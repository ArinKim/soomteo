import React, { useState, useRef, useEffect } from "react";
import {
  SafeAreaView,
  View,
  Text,
  TouchableOpacity,
  Platform,
  PermissionsAndroid,
  Alert,
} from "react-native";
import { styles } from "./styles";
import Voice from "@react-native-voice/voice";
import { useChat } from "./useChat";
import { playTts } from "../lib/TtsPlayer";
import { PERSONA_CONFIG } from "./constants";

export default function CallingScreen({ friend, userId, onEndCall }) {
  console.log("ğŸ“ CallingScreen mounted with:", { friend, userId });

  const [sttResults, setSttResults] = useState([]);
  const [recognizing, setRecognizing] = useState(false);
  const [currentResultId, setCurrentResultId] = useState(null);
  const [aiSpeaking, setAiSpeaking] = useState(false);

  const currentResultIdRef = useRef(null);
  const startingRef = useRef(false);
  const listeningRef = useRef(false);
  const noMatchCountRef = useRef(0);
  const noMatchResetTimerRef = useRef(null);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const silenceTimerRef = useRef(null);

  const roomId = friend?.id ?? null;
  const { uiMessages, sendMessage: wsSendMessage, connected } = useChat(
    roomId,
    userId,
    "CALL"
  );

  // ì¹œêµ¬ì˜ í˜ë¥´ì†Œë‚˜ì— ë§ëŠ” ìŒì„± ì„¤ì • ê°€ì ¸ì˜¤ê¸°
  const voiceConfig = PERSONA_CONFIG[friend?.personality] || {
    voice: "ko-KR-Standard-A",
    rate: 1.0,
    pitch: 0.0,
  };

  // AI ì‘ë‹µ ìˆ˜ì‹  ì‹œ ìë™ TTS ì¬ìƒ
  const lastMessageRef = useRef(null);
  useEffect(() => {
    if (uiMessages.length === 0) return;
    const lastMsg = uiMessages[uiMessages.length - 1];
    
    // AI(ì¹œêµ¬)ë¡œë¶€í„° ì˜¨ ë©”ì‹œì§€ë§Œ TTS ì¬ìƒ
    if (lastMsg.from === "friend" && lastMsg !== lastMessageRef.current) {
      lastMessageRef.current = lastMsg;
      
      console.log("ğŸ”Š AI ì‘ë‹µ ìˆ˜ì‹ , TTS ì¬ìƒ ì‹œì‘:", lastMsg.text);
      console.log("ğŸµ ìŒì„± ì„¤ì •:", voiceConfig);
      
      setAiSpeaking(true);
      
      playTts(lastMsg.text, {
        voice: voiceConfig.voice,
        rate: voiceConfig.rate,
        pitch: voiceConfig.pitch,
      })
        .then(() => {
          console.log("âœ… TTS ì¬ìƒ ì™„ë£Œ");
          setAiSpeaking(false);
        })
        .catch((e) => {
          console.error("âŒ TTS playback error:", e);
          setAiSpeaking(false);
        });
    }
  }, [uiMessages, voiceConfig]);

  const realtimeTranscript =
    sttResults.length > 0 ? sttResults[sttResults.length - 1].text : "";

  const resetSilenceTimer = () => {
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
    }
    silenceTimerRef.current = setTimeout(() => {
      if (recognizing && currentResultId !== null) {
        setCurrentResultId(null);
      }
    }, 2000);
  };

  const requestMicrophonePermission = async () => {
    if (Platform.OS === "android") {
      try {
        const granted = await PermissionsAndroid.request(
          PermissionsAndroid.PERMISSIONS.RECORD_AUDIO
        );
        return granted === PermissionsAndroid.RESULTS.GRANTED;
      } catch (err) {
        console.warn("ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ì¤‘ ì˜¤ë¥˜:", err);
        return false;
      }
    }
    return true;
  };

  const startListening = async () => {
    if (startingRef.current || listeningRef.current) return;
    startingRef.current = true;
    try {
      const ok = await requestMicrophonePermission();
      if (!ok) {
        Alert.alert("ê¶Œí•œ í•„ìš”", "ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì•¼ í•©ë‹ˆë‹¤.");
        startingRef.current = false;
        return;
      }
      await Voice.start("ko-KR");
      listeningRef.current = true;
      setRecognizing(true);
    } catch (err) {
      console.error("STT ì‹œì‘ ì—ëŸ¬:", err);
    } finally {
      startingRef.current = false;
    }
  };

  const stopListening = () => {
    try {
      Voice.stop();
      if (silenceTimerRef.current) clearTimeout(silenceTimerRef.current);
    } catch {}
  };

  useEffect(() => {
    const init = async () => {
      const ok = await requestMicrophonePermission();
      if (ok) await startListening();
    };
    init();

    return () => {
      stopListening();
    };
  }, []);

  // ===== Voice ì´ë²¤íŠ¸ ë°”ì¸ë”© (ê¸°ì¡´ ë™ì¼) =====
  useEffect(() => {
    Voice.onSpeechStart = () => {
      if (listeningRef.current) return;
      setRecognizing(true);
      setIsSpeaking(true);
      const id = Date.now();
      currentResultIdRef.current = id;
      setCurrentResultId(id);
    };

    Voice.onSpeechEnd = () => {
      setRecognizing(false);
      setIsSpeaking(false);
      listeningRef.current = false;
      setTimeout(() => {
        currentResultIdRef.current = null;
        setCurrentResultId(null);
      }, 1200);
    };

    Voice.onSpeechResults = (e) => {
      const finalText = e?.value?.[0] || "";
      if (!finalText.trim()) return;

      console.log("ğŸ¤ STT ìµœì¢… ê²°ê³¼:", finalText);

      setSttResults((prev) => {
        let id = currentResultIdRef.current || Date.now();
        const timestamp = new Date().toLocaleTimeString("ko-KR");
        const index = prev.findIndex((item) => item.id === id);
        if (index !== -1) {
          return prev.map((item, i) =>
            i === index ? { ...item, text: finalText, timestamp } : item
          );
        }
        return [...prev, { id, text: finalText, timestamp }];
      });

      // WebSocketìœ¼ë¡œ ì„œë²„ì— ì „ì†¡
      console.log("ğŸ“¤ ì„œë²„ë¡œ ë©”ì‹œì§€ ì „ì†¡:", finalText);
      wsSendMessage(finalText);

      currentResultIdRef.current = null;
      setCurrentResultId(null);
    };

    Voice.onSpeechPartialResults = (e) => {
      const text = e?.value?.[0] || "";
      if (!text.trim()) {
        resetSilenceTimer();
        return;
      }
      setSttResults((prev) => {
        let id = currentResultIdRef.current;
        const timestamp = new Date().toLocaleTimeString("ko-KR");
        if (!id) {
          id = Date.now();
          currentResultIdRef.current = id;
          setCurrentResultId(id);
        }
        const index = prev.findIndex((item) => item.id === id);
        if (index !== -1) {
          return prev.map((item, i) =>
            i === index ? { ...item, text, timestamp } : item
          );
        }
        return [...prev, { id, text, timestamp }];
      });

      resetSilenceTimer();
    };

    Voice.onSpeechError = () => {
      setRecognizing(false);
      setIsSpeaking(false);
      currentResultIdRef.current = null;
      setCurrentResultId(null);
      listeningRef.current = false;
      startingRef.current = false;
      setTimeout(() => startListening(), 1000);
    };

    return () => {
      Voice.removeAllListeners();
    };
  }, []);

  const handleEndCall = () => {
    onEndCall();
  };

  return (
    <SafeAreaView style={styles.container}>
      <View style={styles.callingContainer}>

        {/* ===== í—¤ë” ===== */}
        <View style={styles.callingHeader}>
          <TouchableOpacity style={styles.closeButton} onPress={handleEndCall}>
            <Text style={styles.closeButtonText}>âœ•</Text>
          </TouchableOpacity>

          <View
            style={[
              styles.avatarLarge,
              {
                backgroundColor: friend?.avatarColor || "#007AFF",
                marginBottom: 12,
              },
            ]}
          >
            <Text style={{ fontSize: 40, fontWeight: "700", color: "#fff" }}>
              {friend?.name?.charAt(0) || "?"}
            </Text>
          </View>

          <Text style={styles.callingTitle}>{friend?.name}</Text>
          <Text style={styles.callingStatus}>{friend?.statusMessage}</Text>
        </View>

        {/* ===== ì¤‘ì•™: ì‹¤ì‹œê°„ ìë§‰ í•œ ì¤„ë§Œ ===== */}
        <View style={styles.currentRecognitionContainer}>
          {aiSpeaking ? (
            <>
              <Text style={[styles.currentRecognitionLabel, { color: "#10b981" }]}>
                ğŸ”Š AI ì‘ë‹µ ì¤‘...
              </Text>
              <Text
                style={[
                  styles.transcriptDisplay,
                  { textAlign: "center", fontSize: 16, paddingHorizontal: 20, color: "#6b7280" },
                ]}
                numberOfLines={3}
                ellipsizeMode="tail"
              >
                {uiMessages.length > 0 && uiMessages[uiMessages.length - 1].from === "friend"
                  ? uiMessages[uiMessages.length - 1].text
                  : ""}
              </Text>
            </>
          ) : recognizing ? (
            <>
              <Text style={styles.currentRecognitionLabel}>ğŸ¤ ì²­ì·¨ ì¤‘...</Text>
              <Text
                style={[
                  styles.transcriptDisplay,
                  { textAlign: "center", fontSize: 18, paddingHorizontal: 20 },
                ]}
                numberOfLines={2}
                ellipsizeMode="tail"
              >
                {realtimeTranscript || ""}
              </Text>
            </>
          ) : (
            <>
              <Text style={styles.currentRecognitionLabel}>ğŸ’¬ ëŒ€ê¸° ì¤‘</Text>
              <TouchableOpacity
                style={styles.manualStartButton}
                onPress={startListening}
              >
                <Text style={styles.manualStartButtonText}>ì²­ì·¨ ì‹œì‘</Text>
              </TouchableOpacity>
            </>
          )}
        </View>

        {/* ===== í•˜ë‹¨: í†µí™” ì¢…ë£Œ ë²„íŠ¼ ===== */}
        <View style={styles.callingActions}>
          <TouchableOpacity onPress={handleEndCall} style={styles.endCallBtn}>
            <Text style={styles.endCallText}>í†µí™” ì¢…ë£Œ</Text>
          </TouchableOpacity>
        </View>

      </View>
    </SafeAreaView>
  );
}
